{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0a9ae7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import random\n",
    "import os\n",
    "import operator\n",
    "import utils \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.python.client import device_lib\n",
    "from tensorflow import keras\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b112ce63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0c45d948",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n",
      "/device:GPU:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-12 19:26:04.756611: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-12 19:26:04.756844: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-12 19:26:04.757015: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-12 19:26:04.757227: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-12 19:26:04.757402: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-12 19:26:04.757526: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /device:GPU:0 with 22816 MB memory:  -> device: 0, name: Quadro RTX 6000, pci bus id: 0000:02:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "print(tf.test.gpu_device_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43a224fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 4007841445872777603\n",
      "xla_global_id: -1\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 23925030912\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 7425289367400722127\n",
      "physical_device_desc: \"device: 0, name: Quadro RTX 6000, pci bus id: 0000:02:00.0, compute capability: 7.5\"\n",
      "xla_global_id: 416903419\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-09 18:51:50.692843: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-09 18:51:50.693143: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-09 18:51:50.693453: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-09 18:51:50.693812: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-09 18:51:50.694154: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-09 18:51:50.694361: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /device:GPU:0 with 22816 MB memory:  -> device: 0, name: Quadro RTX 6000, pci bus id: 0000:02:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82aba1bc",
   "metadata": {},
   "source": [
    "\n",
    "# MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1ed00c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_labels(y_train, y_test):\n",
    "    \"\"\"\n",
    "    Transform label to min equal zero and continuous\n",
    "    For example if we have [1,3,4] --->  [0,1,2]\n",
    "    \"\"\"\n",
    "    # no validation split\n",
    "    # init the encoder\n",
    "    encoder = LabelEncoder()\n",
    "    # concat train and test to fit\n",
    "    y_train_test = np.concatenate((y_train, y_test), axis=0)\n",
    "    # fit the encoder\n",
    "    encoder.fit(y_train_test)\n",
    "    # transform to min zero and continuous labels\n",
    "    new_y_train_test = encoder.transform(y_train_test)\n",
    "    # resplit the train and test\n",
    "    new_y_train = new_y_train_test[0:len(y_train)]\n",
    "    new_y_test = new_y_train_test[len(y_train):]\n",
    "    return new_y_train, new_y_test\n",
    "\n",
    "def check_if_file_exits(file_name):\n",
    "    return os.path.exists(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8b89791f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readucr(filename, delimiter=','):\n",
    "    data = np.loadtxt(filename, delimiter=delimiter)\n",
    "    Y = data[:, 0]\n",
    "    X = data[:, 1:]\n",
    "    return X, Y\n",
    "\n",
    "def read_dataset(path, dataset_name):\n",
    "    x_train, y_train = readucr(path + '/' + dataset_name + '_TRAIN')\n",
    "    x_test, y_test = readucr(path + '/' + dataset_name + '_TEST')\n",
    "    dataset = (x_train.copy(), y_train.copy(), x_test.copy(),\n",
    "                                   y_test.copy())\n",
    "    return dataset\n",
    "\n",
    "def read_all_datasets(root_dir, to_read):\n",
    "    datasets_dict = {}\n",
    "    for dataset_name in to_read:\n",
    "        root_dir_dataset = root_dir + '/UCR_TS_Archive_2015/' + dataset_name + '/'\n",
    "        file_name = root_dir_dataset + dataset_name\n",
    "        x_train, y_train = readucr(file_name + '_TRAIN')\n",
    "        x_test, y_test = readucr(file_name + '_TEST')\n",
    "        datasets_dict[dataset_name] = (x_train.copy(), y_train.copy(), x_test.copy(),\n",
    "                                          y_test.copy())\n",
    "    return datasets_dict\n",
    "\n",
    "def prepare_data(dataset):\n",
    "    x_train = dataset[0]\n",
    "    y_train = dataset[1]\n",
    "    x_test = dataset[2]\n",
    "    y_test = dataset[3]\n",
    "\n",
    "    nb_classes = len(np.unique(np.concatenate((y_train, y_test), axis=0)))\n",
    "\n",
    "    # make the min to zero of labels\n",
    "    y_train, y_test = transform_labels(y_train, y_test)\n",
    "\n",
    "    # save orignal y because later we will use binary\n",
    "    y_true = y_test.astype(np.int64)\n",
    "    y_true_train = y_train.astype(np.int64)\n",
    "    # transform the labels from integers to one hot vectors\n",
    "    enc = sklearn.preprocessing.OneHotEncoder()\n",
    "    enc.fit(np.concatenate((y_train, y_test), axis=0).reshape(-1, 1))\n",
    "    y_train = enc.transform(y_train.reshape(-1, 1)).toarray()\n",
    "    y_test = enc.transform(y_test.reshape(-1, 1)).toarray()\n",
    "    if len(x_train.shape) <= 2:  # if univariate\n",
    "        # add a dimension to make it multivariate with one dimension\n",
    "        x_train = x_train.reshape((x_train.shape[0], x_train.shape[1], 1))\n",
    "        x_test = x_test.reshape((x_test.shape[0], x_test.shape[1], 1))\n",
    "    return x_train, y_train, x_test, y_test, y_true, nb_classes, y_true_train, enc\n",
    "  \n",
    "    \n",
    "    \n",
    "def create_classifier(classifier_name, input_shape, nb_classes, output_directory,\n",
    "                      verbose=False, build=True):\n",
    "    if classifier_name == 'nne':\n",
    "        from classifiers import nne\n",
    "        return nne.Classifier_NNE(output_directory, input_shape,\n",
    "                                  nb_classes, verbose)\n",
    "    if classifier_name == 'inception':\n",
    "        from classifiers import inception\n",
    "        return inception.Classifier_INCEPTION(output_directory, input_shape, nb_classes, verbose,\n",
    "                                              build=build)\n",
    "    \n",
    "    \n",
    "def fit_classifier():\n",
    "    input_shape = x_train.shape[1:]\n",
    "    classifier = create_classifier(classifier_name, input_shape, nb_classes,\n",
    "                                   output_directory)\n",
    "\n",
    "    classifier.fit(x_train, y_train, x_test, y_test, y_true)\n",
    "\n",
    "def create_directory(directory_path):\n",
    "    if os.path.exists(directory_path):\n",
    "        return None\n",
    "    else:\n",
    "        try:\n",
    "            os.makedirs(directory_path)\n",
    "        except:\n",
    "            # in case another machine created the path meanwhile !:(\n",
    "            return None\n",
    "        return directory_path\n",
    "    \n",
    "    \n",
    "def get_xp_val(xp):\n",
    "        if xp == 'batch_size':\n",
    "            xp_arr = [16, 32, 128]\n",
    "        elif xp == 'use_bottleneck':\n",
    "            xp_arr = [False]\n",
    "        elif xp == 'use_residual':\n",
    "            xp_arr = [False]\n",
    "        elif xp == 'nb_filters':\n",
    "            xp_arr = [16, 64]\n",
    "        elif xp == 'depth':\n",
    "            xp_arr = [3, 9]\n",
    "        elif xp == 'kernel_size':\n",
    "            xp_arr = [8, 64]\n",
    "        else:\n",
    "            raise Exception('wrong argument')\n",
    "        return xp_arr\n",
    "    \n",
    "def generate_results_csv(output_file_name, root_dir, clfs, to_read):\n",
    "    res = pd.DataFrame(data=np.zeros((0, 7), dtype=float), index=[],\n",
    "                       columns=['classifier_name', 'dataset_name', 'iteration',\n",
    "                                'precision', 'accuracy', 'recall', 'duration'])\n",
    "    for classifier_name in clfs:\n",
    "        durr = 0.0\n",
    "        for dataset_name in to_read:\n",
    "            output_dir = root_dir + '/results/' + classifier_name + '/' \\\n",
    "                         + dataset_name + '/' + 'df_metrics.csv'\n",
    "            \n",
    "            if not os.path.exists(output_dir):\n",
    "                continue\n",
    "            df_metrics = pd.read_csv(output_dir)\n",
    "            df_metrics['classifier_name'] = classifier_name\n",
    "            df_metrics['dataset_name'] = dataset_name\n",
    "            df_metrics['iteration'] = 0\n",
    "            res = pd.concat((res, df_metrics), axis=0, sort=False)\n",
    "            durr += df_metrics['duration'][0]\n",
    "\n",
    "    res.to_csv(root_dir + '/' + output_file_name, index=False)\n",
    "    res = res.loc[res['classifier_name'].isin(clfs)]\n",
    "\n",
    "    return res\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4383cd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset_rischio(path, strrischio):\n",
    "    with open(path + '/train_set_' + strrischio + '_voronoi_strat.p', 'rb') as f:\n",
    "        x_train = pickle.load(f)[:100]\n",
    "    with open(path + '/train_label_' + strrischio + '_voronoi_strat.p', 'rb') as f:\n",
    "        y_train = pickle.load(f)[:100]\n",
    "    with open(path + '/test_set_' + strrischio + '_voronoi_strat.p', 'rb') as f:\n",
    "        x_test = pickle.load(f)[:100]\n",
    "    with open(path + '/test_label_' + strrischio + '_voronoi_strat.p', 'rb') as f:\n",
    "        y_test = pickle.load(f)[:100]\n",
    "    dataset = (x_train.copy(), y_train.copy(), x_test.copy(),y_test.copy())\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "216b6a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset_tesi(path, strrischio, str_norm, str_overunder):\n",
    "    if str_norm != '':\n",
    "        str_norm = '_' + str_norm\n",
    "    if str_overunder != '':\n",
    "        if strrischio == '2' or strrischio == '5':\n",
    "            str_overunder = '_' + str_overunder +'_40_60'  \n",
    "        else:\n",
    "            str_overunder = '_' + str_overunder +'_50_50'\n",
    "    with open(path + '/training' + strrischio + str_norm + str_overunder +'.p', 'rb') as f:\n",
    "        x_train = pickle.load(f)\n",
    "    with open(path + '/label_training' + strrischio + str_norm + str_overunder +'.p', 'rb') as f:\n",
    "        y_train = pickle.load(f)\n",
    "    with open(path + '/test' + strrischio + str_norm + '.p', 'rb') as f:\n",
    "        x_test = pickle.load(f)\n",
    "    with open(path + '/label_test' + strrischio + '.p', 'rb') as f:\n",
    "        y_test = pickle.load(f)\n",
    "    dataset = (x_train.copy(), y_train.copy(), x_test.copy(),y_test.copy())\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6e5d41ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset_modified(dataset_folder, rischio, scaling, overunder, perc_min = 40, perc_magg = 60):\n",
    "    if overunder != '' and scaling != '':\n",
    "        add_str_ytrain = add_str_xtrain  = '_' + scaling +'_' + overunder + '_' + str(perc_min) + '_' + str(perc_magg)\n",
    "        add_str_xtest  = '_' + scaling\n",
    "        add_str_ytest = ''\n",
    "    elif overunder != '' and scaling == '' :\n",
    "        add_str_ytrain = add_str_xtrain = '_' + overunder + '_' + str(perc_min) + '_' + str(perc_magg)\n",
    "        add_str_ytest = add_str_xtest = ''\n",
    "    elif overunder == '' and scaling != '' :\n",
    "        add_str_ytrain = add_str_ytest = ''\n",
    "        add_str_xtrain = add_str_xtest = '_' + scaling\n",
    "    elif overunder == '' and scaling == '' :\n",
    "        add_str_ytrain = add_str_ytest = add_str_xtrain = add_str_xtest = ''\n",
    "    else:\n",
    "        print('PROBLEMA', overunder, scaling )\n",
    "        return\n",
    "    with open(dataset_folder + '/training' + rischio + add_str_xtrain + '.p', 'rb') as f:\n",
    "        x_train = pickle.load(f)\n",
    "    with open(dataset_folder + '/label_training' + rischio + add_str_ytrain + '.p', 'rb') as f:\n",
    "        y_train = pickle.load(f)\n",
    "    with open(dataset_folder + '/test' + rischio + add_str_xtest + '.p', 'rb') as f:\n",
    "        x_test = pickle.load(f)\n",
    "    with open(dataset_folder + '/label_test' + rischio + add_str_ytest + '.p', 'rb') as f:\n",
    "        y_test = pickle.load(f)\n",
    "    dataset = (x_train.copy(), y_train.copy(), x_test.copy(),y_test.copy())\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6a4700ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_reports(dataset_name, y_test, y_train):\n",
    "\n",
    "    itr_list = ['', '_itr_1', '_itr_2', '_itr_3', '_itr_4']\n",
    "    for itr in itr_list:\n",
    "\n",
    "        reconstructed_model = keras.models.load_model('InceptionTime/results/inception/'+ itr + '/' + dataset_name +'/best_model.hdf5')\n",
    "        y_pred_train = reconstructed_model.predict(x_train)\n",
    "\n",
    "        with open('InceptionTime/results/inception/' + itr + '/' + dataset_name + '/y_pred_train.npy', 'wb') as f:\n",
    "            np.save(f, y_pred_train)\n",
    "\n",
    "        y_pred_train = np.argmax(y_pred_train, axis=1)\n",
    "\n",
    "        with open('InceptionTime/results/inception/' + itr + '/' + dataset_name + '/y_pred.npy', 'rb') as f:\n",
    "            y_pred = np.load(f)\n",
    "\n",
    "        y_pred = np.argmax(y_pred, axis=1)\n",
    "       \n",
    "        with open('InceptionTime/results/inception/' + itr + '/' + dataset_name + '/class_reports', 'w') as f:\n",
    "            f.write('TRAINING ' + dataset_name + '\\n' +\n",
    "                    str(classification_report(y_train, y_pred_train)) + '\\n')\n",
    "            f.write('TEST ' + dataset_name + '\\n' + str(classification_report(y_test, y_pred)) + '\\n')\n",
    "    return\n",
    "    \n",
    "    \n",
    "def nne_reports(dataset_name, y_test, y_train):    \n",
    "    with open('InceptionTime/results/nne/inception-0-1-2-3-4-/' + dataset_name + '/y_pred_train.npy', 'rb') as f:\n",
    "            y_pred_train = np.load(f)\n",
    "\n",
    "    y_pred_train = np.argmax(y_pred_train, axis=1)\n",
    "\n",
    "    with open('InceptionTime/results/nne/inception-0-1-2-3-4-/' + dataset_name + '/y_pred.npy', 'rb') as f:\n",
    "        y_pred = np.load(f)\n",
    "\n",
    "    y_pred = np.argmax(y_pred, axis=1)\n",
    "\n",
    "\n",
    "    with open('/InceptionTime/results/nne/inception-0-1-2-3-4-/' + dataset_name + '/class_reports', 'w') as f:\n",
    "        f.write('TRAINING ' + dataset_name + '\\n' +\n",
    "                str(classification_report(y_train, y_pred_train)) + '\\n')\n",
    "        f.write('TEST ' + dataset_name + '\\n' + str(classification_report(y_test, y_pred)) + '\\n')\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a311a0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_name_f(strrischio, overunder, normst):\n",
    "    str_overunder_dataset = ''\n",
    "    str_norm_dataset = ''\n",
    "    if normst != '':\n",
    "        str_norm_dataset = '_' + normst\n",
    "    if overunder != '':\n",
    "        if strrischio == '2' or strrischio == '5':\n",
    "            str_overunder_dataset = '_' + overunder +'_40_60'  \n",
    "        else:\n",
    "            str_overunder_dataset = '_' + overunder +'_50_50'\n",
    "    dataset_name = 'rischio' + strrischio + str_norm_dataset + str_overunder_dataset\n",
    "    return dataset_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "69f1a80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "risk_dataset = 'rischio2'\n",
    "strrischio = '2'\n",
    "normst = 'norm'\n",
    "overunder = 'randover'\n",
    "dataset_folder = 'datasets_tesi/rischio' + strrischio\n",
    "dataset = read_dataset_modified(dataset_folder, strrischio, normst, overunder, perc_min = '40', perc_magg = '60')\n",
    "x_train, y_train, x_test, y_test, y_true, nb_classes, y_true_train, enc = prepare_data(dataset)\n",
    "itr_list = ['', '_itr_1', '_itr_2', '_itr_3', '_itr_4']\n",
    "itr = itr_list[0]\n",
    "reconstructed_model = keras.models.load_model('InceptionTime/results/inception/'+ itr + '/' + dataset_name +'/best_model.hdf5')\n",
    "y_pred_train = reconstructed_model.predict(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "45d0645c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.0000000e+00, 4.3167083e-09],\n",
       "       [9.9490273e-01, 5.0972179e-03],\n",
       "       [9.9999988e-01, 8.5445194e-08],\n",
       "       ...,\n",
       "       [4.8049332e-11, 1.0000000e+00],\n",
       "       [9.8188599e-16, 1.0000000e+00],\n",
       "       [1.2940050e-11, 1.0000000e+00]], dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190bd24b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.keras code in this scope will run on GPU\n",
      "\t\titer 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-09 18:51:55.284369: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-09 18:51:55.284645: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-09 18:51:55.284848: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-09 18:51:55.285097: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-09 18:51:55.285301: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-09 18:51:55.285437: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22816 MB memory:  -> device: 0, name: Quadro RTX 6000, pci bus id: 0000:02:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_tensor.shape (None, 1800, 1)\n",
      "input_tensor.shape (None, 1800, 128)\n",
      "input_tensor.shape (None, 1800, 128)\n",
      "input_tensor.shape (None, 1800, 128)\n",
      "input_tensor.shape (None, 1800, 128)\n",
      "input_tensor.shape (None, 1800, 128)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-09 18:51:57.920044: I tensorflow/stream_executor/cuda/cuda_dnn.cc:366] Loaded cuDNN version 8201\n",
      "2022-03-09 18:51:58.227565: W tensorflow/stream_executor/gpu/asm_compiler.cc:111] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n",
      "/home/amatteoli/anaconda3/envs/tf-gpu/lib/python3.9/site-packages/keras/engine/functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    }
   ],
   "source": [
    "root_dir = '/InceptionTime/'\n",
    "xps = ['use_bottleneck', 'use_residual', 'nb_filters', 'depth', 'kernel_size', 'batch_size']\n",
    "sys.argv = ['self.py','InceptionTime','']\n",
    "\n",
    "risk_dataset = 'rischio2'\n",
    "strrischio = '2'\n",
    "normst = 'norm'\n",
    "overunder = 'randunder'\n",
    "\n",
    "dataset_name = dataset_name_f(strrischio, overunder, normst)\n",
    "\n",
    "\n",
    "dataset_folder = '/datasets_tesi/rischio' + strrischio\n",
    "\n",
    "with tf.device(\"/device:GPU:0\"):\n",
    "    print(\"tf.keras code in this scope will run on GPU\")\n",
    "    if sys.argv[1] == 'InceptionTime':\n",
    "        classifier_name = 'inception'\n",
    "        nb_iter_ = 5\n",
    "        dataset = read_dataset_modified(dataset_folder, strrischio, normst, overunder, perc_min = '40', perc_magg = '60')\n",
    "        for iter in range(nb_iter_):\n",
    "            print('\\t\\titer', iter)\n",
    "\n",
    "            trr = ''\n",
    "            if iter != 0:\n",
    "                trr = '_itr_' + str(iter)\n",
    "\n",
    "            tmp_output_directory = root_dir + '/results/' + classifier_name + '/' + trr + '/'\n",
    "\n",
    "            x_train, y_train, x_test, y_test, y_true, nb_classes, y_true_train, enc = prepare_data(dataset)\n",
    "\n",
    "            output_directory = tmp_output_directory + dataset_name + '/'\n",
    "\n",
    "            temp_output_directory = create_directory(output_directory)\n",
    "\n",
    "            if temp_output_directory is None:\n",
    "                print('Already_done', tmp_output_directory, dataset_name)\n",
    "                continue\n",
    "\n",
    "            fit_classifier()\n",
    "\n",
    "            print('\\t\\t\\t\\tDONE')\n",
    "\n",
    "            create_directory(output_directory + '/DONE')\n",
    "        \n",
    "        create_reports(dataset_name, np.argmax(y_test, axis=1),np.argmax(y_train, axis=1))\n",
    "        \n",
    "        # run the ensembling of these iterations of Inception\n",
    "        classifier_name = 'nne'\n",
    "\n",
    "        dataset = read_dataset_modified(dataset_folder, strrischio, normst, overunder, perc_min = '40', perc_magg = '60')\n",
    "        \n",
    "        tmp_output_directory = root_dir + '/results/' + classifier_name + '/' \n",
    "\n",
    "        x_train, y_train, x_test, y_test, y_true, nb_classes, y_true_train, enc = prepare_data(dataset)\n",
    "\n",
    "        output_directory = tmp_output_directory + dataset_name + '/'\n",
    "\n",
    "        fit_classifier()\n",
    "\n",
    "        print('\\t\\t\\t\\tDONE')\n",
    "        \n",
    "        \n",
    "        nne_reports(dataset_name, np.argmax(y_test, axis=1),np.argmax(y_train, axis=1))\n",
    "\n",
    "    elif sys.argv[1] == 'InceptionTime_xp':\n",
    "        # this part is for running inception with the different hyperparameters\n",
    "        # listed in the paper\n",
    "        classifier_name = 'inception'\n",
    "        max_iterations = 5\n",
    "\n",
    "        dataset = read_dataset('/UCR_TS_Archive_2015/' + dataset_name, dataset_name)\n",
    "\n",
    "        for xp in xps:\n",
    "\n",
    "            xp_arr = get_xp_val(xp)\n",
    "\n",
    "            print('xp', xp)\n",
    "\n",
    "            for xp_val in xp_arr:\n",
    "                print('\\txp_val', xp_val)\n",
    "\n",
    "                kwargs = {xp: xp_val}\n",
    "\n",
    "                for iter in range(max_iterations):\n",
    "\n",
    "                    trr = ''\n",
    "                    if iter != 0:\n",
    "                        trr = '_itr_' + str(iter)\n",
    "                    print('\\t\\titer', iter)\n",
    "\n",
    "                    output_directory = root_dir + 'results/' + classifier_name + '/' + xp + '/' + str(\n",
    "                        xp_val) + '/' + trr + '/' + dataset_name + '/'\n",
    "\n",
    "\n",
    "                    print('\\t\\t\\tdataset_name', dataset_name)\n",
    "                    x_train, y_train, x_test, y_test, y_true, nb_classes, y_true_train, enc = prepare_data(dataset)\n",
    "\n",
    "                    # check if data is too big for this gpu\n",
    "                    size_data = x_train.shape[0] * x_train.shape[1]\n",
    "\n",
    "                    temp_output_directory = create_directory(output_directory)\n",
    "\n",
    "                    if temp_output_directory is None:\n",
    "                        print('\\t\\t\\t\\t', 'Already_done')\n",
    "                        continue\n",
    "\n",
    "                    input_shape = x_train.shape[1:]\n",
    "                    print(input_shape)\n",
    "                    from classifiers import inception\n",
    "\n",
    "                    classifier = inception.Classifier_INCEPTION(output_directory, input_shape, nb_classes,\n",
    "                                                                    verbose=False, build=True, **kwargs)\n",
    "\n",
    "                    classifier.fit(x_train, y_train, x_test, y_test, y_true)\n",
    "\n",
    "                    # the creation of this directory means\n",
    "                    create_directory(output_directory + '/DONE')\n",
    "\n",
    "                    print('\\t\\t\\t\\t', 'DONE')\n",
    "\n",
    "        # we now need to ensemble each iteration of inception (aka InceptionTime)\n",
    "        classifier_name = 'nne'\n",
    "\n",
    "        dataset = read_dataset('/UCR_TS_Archive_2015/' + dataset_name, dataset_name)\n",
    "\n",
    "        tmp_output_directory = root_dir + '/results/' + classifier_name + '/'\n",
    "\n",
    "        for xp in xps:\n",
    "            xp_arr = get_xp_val(xp)\n",
    "            for xp_val in xp_arr:\n",
    "\n",
    "                clf_name = 'inception/' + xp + '/' + str(xp_val)\n",
    "\n",
    "                x_train, y_train, x_test, y_test, y_true, nb_classes, y_true_train, enc = prepare_data(dataset)\n",
    "\n",
    "                output_directory = tmp_output_directory + dataset_name + '/'\n",
    "                \n",
    "\n",
    "                from classifiers import nne\n",
    "\n",
    "                classifier = nne.Classifier_NNE(output_directory, x_train.shape[1:],\n",
    "                                                    nb_classes, clf_name=clf_name)\n",
    "\n",
    "                classifier.fit(x_train, y_train, x_test, y_test, y_true)\n",
    "\n",
    "    elif sys.argv[1] == 'generate_results_csv':\n",
    "        clfs = []\n",
    "        itr = '-0-1-2-3-4-'\n",
    "        inceptionTime = 'nne/inception'\n",
    "        # add InceptionTime: an ensemble of 5 Inception networks§\n",
    "        clfs.append(inceptionTime + itr)\n",
    "        # add InceptionTime for each hyperparameter study\n",
    "        for xp in xps:\n",
    "            xp_arr = get_xp_val(xp)\n",
    "            for xp_val in xp_arr:\n",
    "                clfs.append(inceptionTime + '/' + xp + '/' + str(xp_val) + itr)\n",
    "        df = generate_results_csv('results.csv', root_dir, clfs, to_read)\n",
    "       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a390daf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "703a2d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "risk_dataset = 'rischio2'\n",
    "strrischio = '2'\n",
    "normst = 'norm'\n",
    "overunder = 'randunder'\n",
    "str_overunder_dataset = ''\n",
    "str_norm_dataset = ''\n",
    "if normst != '':\n",
    "    str_norm_dataset = '_' + normst\n",
    "if overunder != '':\n",
    "    if strrischio == '2' or strrischio == '5':\n",
    "        str_overunder_dataset = '_' + overunder +'_40_60'  \n",
    "    else:\n",
    "        str_overunder_dataset = '_' + overunder +'_50_50'\n",
    "dataset_name = 'rischio' + strrischio + str_norm_dataset + str_overunder_dataset\n",
    "\n",
    "dataset = read_dataset_tesi('/datasets_tesi/rischio' + strrischio , strrischio, normst, overunder)\n",
    "x_train, y_train, x_test, y_test, y_true, nb_classes, y_true_train, enc = prepare_data(dataset)\n",
    "\n",
    "y_train = np.argmax(y_train, axis=1)\n",
    "y_test = np.argmax(y_test, axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}