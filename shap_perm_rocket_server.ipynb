{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0735c331",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Intel(R) Extension for Scikit-learn* enabled (https://github.com/intel/scikit-learn-intelex)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from rocket_functions import generate_kernels, apply_kernels\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "import numpy as np\n",
    "import pickle\n",
    "import math\n",
    "from sklearn.metrics import classification_report\n",
    "import shap\n",
    "import time\n",
    "import csv\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearnex import patch_sklearn\n",
    "patch_sklearn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bad6f32b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"def shannon_entropy(X_training, X_test, pad):\\n    #costruisco un dizionario con la shannon entropy totale per ogni luogo\\n    shannon_tot = {}\\n    #metto l'entropia del padding a 0\\n    shannon_tot[pad] = 0\\n    dataset = np.concatenate((X_training, X_test))\\n    for trajectory in dataset:\\n        #per ogni traiettoria nel training (o dovrei utilizzare anche il test?), posso utilizzare i \\n        #training con over under? cioè se l'errore è piccolo\\n        current_traj = list(filter(lambda a: a != pad, trajectory))\\n        tot_locations = previous_len = len(current_traj)\\n        for location in set(current_traj):\\n            #elimino la locazione corrente e calcolo la nuova lunghezza per l'utente\\n            current_traj = list(filter(lambda a: a != location, current_traj))\\n            current_len = len(current_traj)\\n            #calcolo la shannon entropy per quell'utente e quella traiettoria \\n            prob = (previous_len - current_len)/tot_locations\\n            current_shannon = - prob * math.log(prob,2)  \\n            previous_len = current_len\\n            #la sommo alle altre\\n            shannon_tot[location] = shannon_tot.get(location, 0) + current_shannon\\n    return(shannon_tot)\\n\\n#entropia della locazione, \\n\\ndef funct_mask(x, pad, shannon_tot):\\n    x_mask = [0] * len(x)\\n    traj_len = len(list(filter(lambda a: a != pad, x)))\\n    #individuo il numero che da 30% locazioni\\n    n_feat = int((30 * traj_len)/100)\\n    enum = list(enumerate(x))\\n    #lista degli indici delle posizioni da mettere a uno nella maschera\\n    ones = [x for x,y in sorted(enum, key = lambda e: shannon_tot[e[1]], reverse = True)[:n_feat]]\\n    for index in ones:\\n        x_mask[index] = 1\\n    return x_mask\\n\\ndef custom_masker(mask, x):\\n    return (x * mask).reshape(1,len(x))\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def normalization(X_training, X_test):   \n",
    "    dataref = np.concatenate((X_training, X_test), axis = 0)\n",
    "    dataref_plain = dataref.reshape(dataref.shape[0]*dataref.shape[1], 1)\n",
    "    X_training_plain = X_training.reshape(X_training.shape[0]*X_training.shape[1], 1)\n",
    "    X_test_plain = X_test.reshape(X_test.shape[0]*X_test.shape[1], 1)\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(dataref_plain)\n",
    "    X_training_norm = scaler.transform(X_training_plain)\n",
    "    X_test_norm = scaler.transform(X_test_plain)\n",
    "    X_training_norm = X_training_norm.reshape(X_training.shape[0], X_training.shape[1])\n",
    "    X_test_norm = X_test_norm.reshape(X_test.shape[0], X_test.shape[1])\n",
    "    return X_training_norm, X_test_norm\n",
    "\n",
    "\n",
    "def normalization_estesa(X_training, X_test):   \n",
    "    dataref = np.concatenate((X_training, X_test), axis = 0)\n",
    "    dataref_plain = dataref.reshape(dataref.shape[0]*dataref.shape[1], 1)\n",
    "    X_training_plain = X_training.reshape(X_training.shape[0]*X_training.shape[1], 1)\n",
    "    X_test_plain = X_test.reshape(X_test.shape[0]*X_test.shape[1], 1)\n",
    "    massimo = max(dataref_plain)\n",
    "    minimo = min(dataref_plain)\n",
    "    X_training_norm_plain = (X_training_plain - minimo)/(massimo - minimo)\n",
    "    X_training_norm = X_training_norm_plain.reshape(X_training.shape[0], X_training.shape[1])\n",
    "    return X_training_norm, massimo, minimo\n",
    "\n",
    "\n",
    "def normalization_shap(x):\n",
    "    x_norm = (x - minimo)/(massimo - minimo)\n",
    "    return x_norm\n",
    "\n",
    "\n",
    "def read_dataset_modified(dataset_folder, rischio, scaling, overunder, perc_min = 40, perc_magg = 60):\n",
    "    if overunder != '' and scaling != '':\n",
    "        add_str_ytrain = add_str_xtrain  = '_' + scaling +'_' + overunder + '_' + str(perc_min) + '_' + str(perc_magg)\n",
    "        add_str_xtest  = '_' + scaling\n",
    "        add_str_ytest = ''\n",
    "    elif overunder != '' and scaling == '' :\n",
    "        add_str_ytrain = add_str_xtrain = '_' + overunder + '_' + str(perc_min) + '_' + str(perc_magg)\n",
    "        add_str_ytest = add_str_xtest = ''\n",
    "    elif overunder == '' and scaling != '' :\n",
    "        add_str_ytrain = add_str_ytest = ''\n",
    "        add_str_xtrain = add_str_xtest = '_' + scaling\n",
    "    elif overunder == '' and scaling == '' :\n",
    "        add_str_ytrain = add_str_ytest = add_str_xtrain = add_str_xtest = ''\n",
    "    else:\n",
    "        print('PROBLEMA', overunder, scaling )\n",
    "        return\n",
    "    with open(dataset_folder + '/training' + rischio + add_str_xtrain + '.p', 'rb') as f:\n",
    "        x_train = pickle.load(f)\n",
    "        X_training = x_train.astype(np.float64)\n",
    "    with open(dataset_folder + '/label_training' + rischio + add_str_ytrain + '.p', 'rb') as f:\n",
    "        y_train = pickle.load(f)\n",
    "        Y_training = y_train.astype(np.int32)\n",
    "    with open(dataset_folder + '/test' + rischio + add_str_xtest + '.p', 'rb') as f:\n",
    "        x_test = pickle.load(f)\n",
    "        X_test = x_test.astype(np.float64)\n",
    "    with open(dataset_folder + '/label_test' + rischio + add_str_ytest + '.p', 'rb') as f:\n",
    "        y_test = pickle.load(f)\n",
    "        Y_test = y_test.astype(np.int32)\n",
    "    return X_training, Y_training, X_test, Y_test\n",
    "\n",
    "def dataset_name_f(strrischio, overunder, normst):\n",
    "    str_overunder_dataset = ''\n",
    "    str_norm_dataset = ''\n",
    "    if normst != '':\n",
    "        str_norm_dataset = '_' + normst\n",
    "    if overunder != '':\n",
    "        if strrischio == '2' or strrischio == '5':\n",
    "            str_overunder_dataset = '_' + overunder +'_40_60'  \n",
    "        else:\n",
    "            str_overunder_dataset = '_' + overunder +'_50_50'\n",
    "    dataset_name = 'rischio' + strrischio + str_norm_dataset + str_overunder_dataset\n",
    "    return dataset_name\n",
    "\n",
    "\n",
    "def choose_kernels_perc(rischio):\n",
    "    if rischio == '2' or rischio == '3':\n",
    "        f = open('/home/amatteoli/rocket-prove/MODELLI_SCELTI/kernels_2_3.pckl', 'rb')\n",
    "        kernels = pickle.load(f)\n",
    "        f.close()   \n",
    "    elif rischio == '4':\n",
    "        f = open('/home/amatteoli/rocket-prove/MODELLI_SCELTI/kernels_4.pckl', 'rb')\n",
    "        kernels = pickle.load(f)\n",
    "        f.close()\n",
    "    elif rischio == '5':\n",
    "        f = open('/home/amatteoli/rocket-prove/MODELLI_SCELTI/kernels_5.pckl', 'rb')\n",
    "        kernels = pickle.load(f)\n",
    "        f.close()\n",
    "    else:\n",
    "        print('rischio inesistente')\n",
    "    \n",
    "    if rischio == '2' or rischio == '5':\n",
    "        perc_min = 40\n",
    "        perc_mag = 60\n",
    "    else:\n",
    "        perc_mag = perc_min = 50\n",
    "    return kernels, perc_min, perc_mag\n",
    "\n",
    "def pred(x):\n",
    "    x_transform = apply_kernels(x, kernels)\n",
    "    x_transform = normalization_shap(x_transform)\n",
    "    predictions = classifier.predict(x_transform)\n",
    "    return predictions\n",
    "\n",
    "def pred_prob(x):\n",
    "    x_transform = apply_kernels(x, kernels)\n",
    "    x_transform = normalization_shap(x_transform)\n",
    "    prob_pred = classifier.predict_proba(x_transform)\n",
    "    return prob_pred\n",
    "\n",
    "def pred_prob_RC(x):\n",
    "    x_transform = apply_kernels(x, kernels)\n",
    "    x_transform = normalization_shap(x_transform)\n",
    "    d = classifier.decision_function(x_transform)\n",
    "    probs = np.array([np.exp(-d) / (1 + np.exp(-d)), np.exp(d) / (1 + np.exp(d))]).flatten()\n",
    "    return(probs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76a661ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#construisco un dizionario che contiene il totale di volte che un luogo è stato visitato:\n",
    "def dict_tot_visits(X_training, X_test, pad):\n",
    "    tot_visits = {}\n",
    "    dataset = np.concatenate((X_training, X_test))\n",
    "    for trajectory in dataset:\n",
    "        for location in trajectory:\n",
    "            if location != pad:\n",
    "                tot_visits[location] = tot_visits.get(location, 0) + 1\n",
    "    return tot_visits\n",
    "                \n",
    "    \n",
    "def shannon_entropy(X_training, X_test,  pad):\n",
    "    #costruisco un dizionario con la shannon entropy totale per ogni luogo \n",
    "    shannon_tot = {}\n",
    "    shannon_tot[pad] = float('Inf')\n",
    "    tot_visits = dict_tot_visits(X_training, X_test, pad)\n",
    "    #metto l'entropia del padding a Inf\n",
    "    dataset = np.concatenate((X_training, X_test))\n",
    "    for trajectory in dataset:\n",
    "        #per ogni traiettoria \n",
    "        #tolgo il padding\n",
    "        current_traj = list(filter(lambda a: a != pad, trajectory))\n",
    "        for location in set(current_traj):\n",
    "            occurrences = current_traj.count(location)\n",
    "            #calcolo la shannon entropy per quell'utente in quella posizione\n",
    "            prob = occurrences/tot_visits[location]\n",
    "            current_shannon = - prob * math.log(prob,2)  \n",
    "            #la sommo alle altre\n",
    "            shannon_tot[location] = shannon_tot.get(location, 0) + current_shannon\n",
    "    return(shannon_tot)\n",
    "\n",
    "\n",
    "def transformer_shannon_tot_coord(shannon_tot, scaling_to_int_ref):\n",
    "    shannon_tot_int = {}\n",
    "    for key in shannon_tot:\n",
    "        shannon_tot_int[scaling_to_int_ref[key]] = shannon_tot[key]\n",
    "    return shannon_tot_int \n",
    "        \n",
    "\n",
    "#maschera per il singolo utente traj x\n",
    "def funct_mask(x, pad, shannon_tot_adapted):\n",
    "    x_mask = [0] * len(x)\n",
    "    traj_len = len(list(filter(lambda a: a != pad, x)))\n",
    "    #individuo il numero che da 30% locazioni\n",
    "    n_feat = math.ceil((30 * traj_len)/100)\n",
    "    enum = list(enumerate(x))\n",
    "    #lista degli indici delle posizioni da mettere a uno nella maschera\n",
    "    ones = [x for x,y in sorted(enum, key = lambda e: shannon_tot_adapted[e[1]])[:n_feat]]\n",
    "    for index in ones:\n",
    "        x_mask[index] = 1\n",
    "    return x_mask\n",
    "\n",
    "\n",
    "def custom_masker(mask, x):\n",
    "    return (x * mask).reshape(1,len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2de5cc5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_kernels_pre_shap(X_training, X_test, kernels):\n",
    "    inizio = time.time()\n",
    "    X_training_transform = apply_kernels(X_training, kernels)\n",
    "    fine = time.time()\n",
    "    print('Running kernels su train:' , fine - inizio)\n",
    "    inizio = time.time()\n",
    "    X_test_transform = apply_kernels(X_test, kernels)\n",
    "    fine = time.time()\n",
    "    print('Running kernels su test:' , fine - inizio)\n",
    "    _, massimo, minimo = normalization_estesa(X_training_transform, X_test_transform)\n",
    "    return(massimo, minimo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "939e72bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "overunder = '' #'', 'randunder', 'IHTunder', 'randover', 'smote', \n",
    "scaling = 'norm' #'', 'norm', 'std'\n",
    "rischio = '4'\n",
    "classifier_type = 'SDG'\n",
    "\n",
    "dataset = dataset_name_f(rischio, overunder, scaling)\n",
    "\n",
    "dataset_folder = '/datasets_tesi/pad_incima/rischio' + rischio\n",
    "models_folder = '/rocket-prove/MODELLI_SCELTI/voronoi/'\n",
    "classifier_name = classifier_type + '_' + dataset + '_modello.sav'\n",
    "shap_folder = '/SHAP/'\n",
    "tempo_shapley = 'shapley_values_time.txt'\n",
    "shapley_file = 'shap_values_rocket_' + classifier_type + '_' + dataset\n",
    "\n",
    "kernels, perc_min, perc_mag = choose_kernels_perc(rischio)\n",
    "\n",
    "X_training, Y_training, X_test, Y_test = read_dataset_modified(dataset_folder, rischio, scaling,\n",
    "                                                                    overunder, perc_min, perc_mag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4b368b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pad = X_training[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "846584fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(shap_folder + 'shannon_tot_int_general2.p', 'rb') as handle:\n",
    "    shannon_tot_int = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "842c80e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = shap_folder + 'coord_mappings/'\n",
    "risk_dataset = 'rischio' + rischio\n",
    "\n",
    "\n",
    "if scaling != '':\n",
    "    with open(path + 'map_' + scaling + '_to_int_' + risk_dataset + '.p', 'rb') as f:\n",
    "            norm_to_int = pickle.load(f)\n",
    "\n",
    "    shannon_tot_adapted = {}\n",
    "\n",
    "    for key_norm, key_int in norm_to_int.items():\n",
    "        shannon_tot_adapted[key_norm] = shannon_tot_int[key_int]\n",
    "\n",
    "    del shannon_tot_int\n",
    "    del norm_to_int\n",
    "else:\n",
    "    shannon_tot_adapted = shannon_tot_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69049e60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running kernels su train: 71.63004660606384\n",
      "Running kernels su test: 18.072691679000854\n"
     ]
    }
   ],
   "source": [
    "classifier = pickle.load(open(models_folder + classifier_name, 'rb'))\n",
    "massimo, minimo = run_kernels_pre_shap(X_training, X_test, kernels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3cbdc9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Permutation explainer: 2it [00:40, 40.38s/it]                                                                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Permutation explainer: 2it [00:39, 39.80s/it]                                                                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "inizio = time.time()\n",
    "for index,x in enumerate(X_test):\n",
    "    mask = funct_mask(x, pad, shannon_tot_adapted)\n",
    "    explainer = shap.Explainer(pred_prob, custom_masker)\n",
    "    shap_values = explainer.shap_values(np.array([x]))\n",
    "    shap_tuple = (index,shap_values)\n",
    "    with open(shap_folder + shapley_file + '.p', 'ab') as fp:\n",
    "        pickle.dump(shap_tuple,fp)\n",
    "    del explainer\n",
    "    print(index)\n",
    "fine = time.time()\n",
    "tempo = fine - inizio\n",
    "with open(shap_folder + tempo_shapley , 'a') as f:\n",
    "    f.write(shapley_file + '  ' + str(tempo))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}